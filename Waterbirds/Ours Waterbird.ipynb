{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from math import log, sqrt, pi\n",
    "import argparse\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable, grad\n",
    "from scipy import linalg as la\n",
    "import math\n",
    "import torchvision.transforms as tvt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import utils\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import trange\n",
    "from transformers import ViTModel\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 224\n",
    "batch_size = 64\n",
    "torch.set_num_threads(1)   # Sets the number of threads used for intra-operations\n",
    "torch.set_num_interop_threads(1)   # Sets the number of threads used for inter-operations\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Changes the seed for reproducibility. \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    \n",
    "class ConfounderDataset(Dataset):\n",
    "    def __init__(self, root_dir,\n",
    "                 target_name, confounder_names,\n",
    "                 model_type=None, augment_data=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == 'Train':\n",
    "            return len(self.training_sample)\n",
    "        if self.split == 'Valid':\n",
    "            return len(self.valid_sample)\n",
    "        if self.split == 'Test':\n",
    "            return len(self.test_sample)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == 'Train': \n",
    "            y = self.training_sample_y_array[idx]\n",
    "            a = self.training_sample_confounder_array[idx]\n",
    "            img_filename = os.path.join(\n",
    "                self.data_dir,\n",
    "                self.training_sample[idx])       \n",
    "            img = Image.open(img_filename).convert('RGB')\n",
    "            img = self.train_transform(img)\n",
    "            x = img\n",
    "            \n",
    "        if self.split == 'Valid': \n",
    "            y = self.valid_sample_y_array[idx]\n",
    "            a = self.valid_sample_confounder_array[idx]\n",
    "            img_filename = os.path.join(\n",
    "                self.data_dir,\n",
    "                self.valid_sample[idx])       \n",
    "            img = Image.open(img_filename).convert('RGB')\n",
    "            img = self.eval_transform(img)\n",
    "            x = img\n",
    "            \n",
    "        if self.split == 'Test': \n",
    "            y = self.test_sample_y_array[idx]\n",
    "            a = self.test_sample_confounder_array[idx]\n",
    "            img_filename = os.path.join(\n",
    "                self.data_dir,\n",
    "                self.test_sample[idx])       \n",
    "            img = Image.open(img_filename).convert('RGB')\n",
    "            img = self.eval_transform(img)\n",
    "            x = img\n",
    "        return x,y,a\n",
    "\n",
    "    \n",
    "    \n",
    "class CUBDataset(ConfounderDataset):\n",
    "    \"\"\"\n",
    "    CUB dataset (already cropped and centered).\n",
    "    Note: metadata_df is one-indexed.\n",
    "    \"\"\"\n",
    "    def __init__(self, fold_dir, split):\n",
    "        self.data_dir = fold_dir\n",
    "        self.split = split\n",
    "\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.data_dir} does not exist yet. Please generate the dataset first.')\n",
    "\n",
    "        # Read in metadata\n",
    "        self.metadata_df = pd.read_csv(\n",
    "            os.path.join(self.data_dir, 'metadata.csv'))\n",
    "\n",
    "        # Get the y values\n",
    "        self.y_array = self.metadata_df['y'].values\n",
    "        self.n_classes = 2\n",
    "\n",
    "        # We only support one confounder for CUB for now\n",
    "        self.confounder_array = self.metadata_df['place'].values\n",
    "        self.n_confounders = 1\n",
    "        \n",
    "        # Extract filenames and splits\n",
    "        self.filename_array = self.metadata_df['img_filename'].values\n",
    "        self.split_array = self.metadata_df['split'].values\n",
    "\n",
    "        self.training_sample = self.filename_array[self.split_array == 0]\n",
    "        self.training_sample_y_array = self.y_array[self.split_array == 0]\n",
    "        self.training_sample_confounder_array =self.confounder_array[self.split_array == 0]\n",
    "        \n",
    "        self.valid_sample = self.filename_array[self.split_array == 1]\n",
    "        self.valid_sample_y_array = self.y_array[self.split_array == 1]\n",
    "        self.valid_sample_confounder_array =self.confounder_array[self.split_array == 1]\n",
    "        \n",
    "        self.test_sample = self.filename_array[self.split_array == 2]\n",
    "        self.test_sample_y_array = self.y_array[self.split_array == 2]\n",
    "        self.test_sample_confounder_array =self.confounder_array[self.split_array == 2]\n",
    "        \n",
    "        # Set transform\n",
    "        self.train_transform = get_transform_cub(aug = True)\n",
    "        self.eval_transform = get_transform_cub(aug = False)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def get_transform_cub(aug):\n",
    "    if aug == False:\n",
    "        transform=tvt.Compose([tvt.Resize((256,256)),\n",
    "                               tvt.CenterCrop((224,224)),\n",
    "                                tvt.ToTensor(),\n",
    "                                tvt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                  \n",
    "                                ])\n",
    "    if aug == True:\n",
    "        transform=tvt.Compose([tvt.Resize((256,256)),\n",
    "                               tvt.RandomResizedCrop(\n",
    "                                    (224,224),\n",
    "                                    scale=(0.7, 1.0),\n",
    "                                    ratio=(0.75, 1.3333333333333333),\n",
    "                                    interpolation=2),\n",
    "                                tvt.RandomHorizontalFlip(),\n",
    "                                tvt.ToTensor(),\n",
    "                                tvt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                  \n",
    "                                ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "seed_everything(2048)\n",
    "\n",
    "fold_dir = r'../../waterbird'\n",
    "\n",
    "training_dataset = CUBDataset(fold_dir, 'Train')\n",
    "valid_dataset = CUBDataset(fold_dir, 'Valid')\n",
    "test_dataset = CUBDataset(fold_dir, 'Test')\n",
    "\n",
    "training_data_loader  = torch.utils.data.DataLoader(dataset = training_dataset,\n",
    "                                                batch_size= batch_size,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=0)\n",
    "\n",
    "valid_data_loader  = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
    "                                                batch_size= batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=0)\n",
    "\n",
    "test_data_loader  = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                                batch_size= batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=0)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32987999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "    \n",
    "def compute_variance(g1, g2, g3, g4):\n",
    "    g1_tensor = torch.from_numpy(g1)\n",
    "    g2_tensor = torch.from_numpy(g2)\n",
    "    g3_tensor = torch.from_numpy(g3)\n",
    "    g4_tensor = torch.from_numpy(g4)\n",
    "    sample_feature = np.concatenate((g1_tensor, g2_tensor, g3_tensor, g4_tensor), axis=0)\n",
    "    sample_tensor = torch.from_numpy(sample_feature)\n",
    "    \n",
    "    cov_g1 = torch.cov(g1_tensor.T)\n",
    "    cov_g2 = torch.cov(g2_tensor.T)\n",
    "    cov_g3 = torch.cov(g3_tensor.T)\n",
    "    cov_g4 = torch.cov(g4_tensor.T)\n",
    "    \n",
    "    cov_all = torch.cov(sample_tensor.T)\n",
    "    g1_trace = torch.trace(cov_g1)\n",
    "    g2_trace = torch.trace(cov_g2)\n",
    "    g3_trace = torch.trace(cov_g3)\n",
    "    g4_trace = torch.trace(cov_g4)\n",
    "    \n",
    "    all_trace = torch.trace(cov_all)\n",
    "    \n",
    "    print(cov_g1.shape)\n",
    "    print(cov_all.shape)\n",
    "    print(\"g1 trace\",g1_trace,\"g2 trace\",g2_trace,\"g3 trace\",g3_trace,\"g4 trace\",g4_trace,\"ALL trace\",all_trace)\n",
    "    print(\"g1\",g1_trace/all_trace,\"g2\",g2_trace/all_trace,\"g3\",g3_trace/all_trace,\"g4\",g4_trace/all_trace)\n",
    "\n",
    "    \n",
    "def neural_collapse():\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    model.load_state_dict(torch.load('ICML_weight/WB_ResNet_50_model_epoch_100.pth'))\n",
    "    model.fc = nn.Identity()\n",
    "    model.to(device)   \n",
    "    feature_list = []\n",
    "    feature_y_0_a0 = []\n",
    "    feature_y_0_a1 = []\n",
    "    feature_y_1_a0 = []\n",
    "    feature_y_1_a1 = []\n",
    "    \n",
    "    for step, (test_input, test_target, sensitive) in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n",
    "        with torch.no_grad():\n",
    "            test_input = test_input.to(device)\n",
    "            test_pred_ = model(test_input)\n",
    "            label = test_target.squeeze().detach().cpu()\n",
    "            mask_00 = ((label == 0) & (sensitive == 0))\n",
    "            mask_01 = ((label == 0) & (sensitive == 1))\n",
    "            mask_10 = ((label == 1) & (sensitive == 0))\n",
    "            mask_11 = ((label == 1) & (sensitive == 1))\n",
    "            g1 = test_pred_[mask_00]\n",
    "            g2 = test_pred_[mask_01]\n",
    "            g3 = test_pred_[mask_10]\n",
    "            g4 = test_pred_[mask_11]\n",
    "            feature_y_0_a0.extend(g1.detach().cpu().numpy())\n",
    "            feature_y_0_a1.extend(g2.detach().cpu().numpy())\n",
    "            feature_y_1_a0.extend(g3.detach().cpu().numpy())\n",
    "            feature_y_1_a1.extend(g4.detach().cpu().numpy())\n",
    "\n",
    "            \n",
    "    feature_g1 = np.array(feature_y_0_a0)\n",
    "    feature_g2 = np.array(feature_y_0_a1)\n",
    "    feature_g3 = np.array(feature_y_1_a0)\n",
    "    feature_g4 = np.array(feature_y_1_a1)\n",
    "    print(feature_g1.shape, feature_g2.shape,feature_g3.shape,feature_g4.shape)\n",
    "    \n",
    "    compute_variance(feature_g1,feature_g2,feature_g3,feature_g4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    epoch =30\n",
    "    weight_decay=1e-3\n",
    "    init_lr=1e-4\n",
    "    momentum_decay = 0.9\n",
    "    schedule = True\n",
    "    resnet50 = models.resnet50(pretrained=True)\n",
    "    resnet50.fc = nn.Identity()\n",
    "    num_classes = 2 \n",
    "    classifier = nn.Linear(2048, num_classes)\n",
    "    model = resnet50.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "    resnet50_parameters = model.parameters()\n",
    "    classifier_parameters = classifier.parameters()\n",
    "    combined_parameters = list(resnet50_parameters) + list(classifier_parameters)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    mean_criterion = nn.MSELoss()\n",
    "    acc = 0\n",
    "    optimizer = optim.SGD(combined_parameters, lr=init_lr, momentum=momentum_decay, weight_decay = weight_decay)\n",
    "    optimizer_2 = optim.SGD(model.parameters(), lr=init_lr, momentum=momentum_decay, weight_decay = weight_decay)\n",
    "    \n",
    "    if schedule == True:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max= epoch)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    for epoches in range(epoch):\n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            feature_y_0_a0 = []\n",
    "            feature_y_0_a1 = []\n",
    "            feature_y_1_a0 = []\n",
    "            feature_y_1_a1 = []\n",
    "            loss00 = 0\n",
    "            loss01 = 0\n",
    "            loss10 = 0\n",
    "            loss11 = 0\n",
    "            \n",
    "            with torch.no_grad(): \n",
    "                for step, (valid_input, valid_target, validsensitive) in enumerate(valid_data_loader):\n",
    "                    valid_input = valid_input.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        valid_feature = model(valid_input)\n",
    "                        label = valid_target.squeeze().detach().cpu()\n",
    "                        mask_00 = ((label == 0) & (validsensitive == 0))\n",
    "                        mask_01 = ((label == 0) & (validsensitive == 1))\n",
    "                        mask_10 = ((label == 1) & (validsensitive == 0))\n",
    "                        mask_11 = ((label == 1) & (validsensitive == 1))\n",
    "                        g1 = valid_feature[mask_00]\n",
    "                        g2 = valid_feature[mask_01]\n",
    "                        g3 = valid_feature[mask_10]\n",
    "                        g4 = valid_feature[mask_11]\n",
    "                        feature_y_0_a0.extend(g1.detach().cpu().numpy())\n",
    "                        feature_y_0_a1.extend(g2.detach().cpu().numpy())\n",
    "                        feature_y_1_a0.extend(g3.detach().cpu().numpy())\n",
    "                        feature_y_1_a1.extend(g4.detach().cpu().numpy())\n",
    "\n",
    "                feature_g1 = np.array(feature_y_0_a0)\n",
    "                feature_g4 = np.array(feature_y_1_a1)\n",
    "                feature_g1_tensor = torch.from_numpy(feature_g1)\n",
    "                feature_g4_tensor = torch.from_numpy(feature_g4)\n",
    "\n",
    "                mu_1 = torch.mean(feature_g1_tensor, 0)\n",
    "                mu_1 = mu_1 /torch.norm(mu_1)\n",
    "                mu_2 = torch.mean(feature_g4_tensor, 0)\n",
    "                mu_2 = mu_2 /torch.norm(mu_2)\n",
    "                weight = torch.cat((mu_1.unsqueeze(0), mu_2.unsqueeze(0)), 0)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    classifier.weight = nn.Parameter(weight)\n",
    "\n",
    "            for train_input, train_target, sensitive in tepoch:\n",
    "                train_input = train_input.to(device)\n",
    "                label = train_target.detach().cpu() \n",
    "                one_hot_labels = F.one_hot(train_target, num_classes=2)\n",
    "                train_target = one_hot_labels.float().to(device)\n",
    "                \n",
    "                feature = model(train_input)\n",
    "                classifier = classifier.to(device)\n",
    "                outputs  = classifier(feature)\n",
    "              \n",
    "                mask_00 = ((label== 0) & (sensitive == 0))\n",
    "                mask_01 = ((label == 0) & (sensitive == 1))\n",
    "                mask_10 = ((label == 1) & (sensitive == 0))\n",
    "                mask_11 = ((label == 1) & (sensitive == 1))\n",
    "   \n",
    "                count_00 = mask_00.sum()\n",
    "                count_01 = mask_01.sum()\n",
    "                count_10 = mask_10.sum()\n",
    "                count_11 = mask_11.sum()\n",
    "                    \n",
    "                if count_01==0 or count_10==0 or count_00 == 0 or count_11 == 0:\n",
    "                    continue    \n",
    "                g1_f = feature[mask_00]\n",
    "                g2_f = feature[mask_01]    \n",
    "                mu1 = torch.mean(g1_f, 0)\n",
    "                mu2 = torch.mean(g2_f, 0)\n",
    "                    \n",
    "                g3_f = feature[mask_10]\n",
    "                g4_f = feature[mask_11]\n",
    "                mu3 = torch.mean(g3_f, 0)\n",
    "                mu4 = torch.mean(g4_f, 0)\n",
    "                \n",
    "                loss_mean = mean_criterion(mu1, mu2) + mean_criterion(mu3, mu4)\n",
    "                \n",
    "                \n",
    "                if count_00 > 0:\n",
    "                    loss_00 = criterion(outputs[mask_00], train_target[mask_00])\n",
    "                    loss00 += loss_00.item()\n",
    "                else:\n",
    "                    loss_00 = torch.tensor(0)\n",
    "                    \n",
    "                if count_01 > 0:\n",
    "                    loss_01 = criterion(outputs[mask_01], train_target[mask_01])\n",
    "                    loss01 += loss_01.item()\n",
    "                else:\n",
    "                    loss_01 = torch.tensor(0)\n",
    "                    \n",
    "                if count_10 > 0:\n",
    "                    loss_10 = criterion(outputs[mask_10], train_target[mask_10])\n",
    "                    loss10 += loss_10.item()\n",
    "                else:\n",
    "                    loss_10 = torch.tensor(0)\n",
    "                    \n",
    "                if count_11 > 0:\n",
    "                    loss_11 = criterion(outputs[mask_11], train_target[mask_11])\n",
    "                    loss11 += loss_11.item()\n",
    "                else:\n",
    "                    loss_11 = torch.tensor(0)\n",
    "\n",
    "                loss = loss_00 + loss_01 + loss_10 + loss_11 + loss_mean               \n",
    "                tepoch.set_postfix(ut_loss = loss.item())                   \n",
    "                optimizer_2.zero_grad()    \n",
    "                loss.backward()\n",
    "                optimizer_2.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)\n",
    "            if schedule:\n",
    "                scheduler.step()\n",
    "                \n",
    "        print(\"loss g1 (label=0, sensitive=0):\",loss00 )\n",
    "        print(\"loss g2 (label=0, sensitive=1):\",loss01 )\n",
    "        print(\"loss g3 (label=1, sensitive=0):\",loss10 )\n",
    "        print(\"loss g4 (label=1, sensitive=1):\",loss11 )\n",
    "        print('mean loss:', loss_mean.item())\n",
    "        \n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_gt = []\n",
    "        sense_gt = []\n",
    "        female_predic = []\n",
    "        female_gt = []\n",
    "        male_predic = []\n",
    "        male_gt = []\n",
    "        correct_00, total_00 = 0, 0\n",
    "        correct_01, total_01 = 0, 0\n",
    "        correct_10, total_10 = 0, 0\n",
    "        correct_11, total_11 = 0, 0\n",
    "\n",
    "    # Evaluate on test set.\n",
    "        for step, (test_input, test_target, sensitive) in enumerate(test_data_loader):\n",
    "            test_input = test_input.to(device)\n",
    "            test_target = test_target.to(device)\n",
    "\n",
    "            gt = test_target.detach().cpu().numpy()\n",
    "            sen = sensitive.detach().cpu().numpy()\n",
    "            test_gt.extend(gt)\n",
    "            sense_gt.extend(sen)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_fature = model(test_input)\n",
    "                test_pred_ = classifier(test_fature)\n",
    "                _, predic = torch.max(test_pred_.data, 1)\n",
    "                predic = predic.detach().cpu()\n",
    "                test_pred.extend(predic.numpy())\n",
    "                label = test_target.squeeze().detach().cpu()\n",
    "                mask_00 = ((label == 0) & (sensitive == 0))\n",
    "                mask_01 = ((label == 0) & (sensitive == 1))\n",
    "                mask_10 = ((label == 1) & (sensitive == 0))\n",
    "                mask_11 = ((label == 1) & (sensitive == 1))\n",
    "\n",
    "                correct_00 += (predic[mask_00] == label[mask_00]).float().sum().item()\n",
    "                total_00 += mask_00.float().sum().item()\n",
    "\n",
    "                correct_01 += (predic[mask_01] == label[mask_01]).float().sum().item()\n",
    "                total_01 += mask_01.float().sum().item()\n",
    "\n",
    "                correct_10 += (predic[mask_10] == label[mask_10]).float().sum().item()\n",
    "                total_10 += mask_10.float().sum().item()\n",
    "\n",
    "                correct_11 += (predic[mask_11] == label[mask_11]).float().sum().item()\n",
    "                total_11 += mask_11.float().sum().item() \n",
    "        acc_00 = correct_00 / total_00\n",
    "        acc_01 = correct_01 / total_01\n",
    "        acc_10 = correct_10 / total_10\n",
    "        acc_11 = correct_11 / total_11\n",
    "\n",
    "        print(f'Accuracy for y=0, s=0: {acc_00}')\n",
    "        print(f'Accuracy for y=0, s=1: {acc_01}')\n",
    "        print(f'Accuracy for y=1, s=0: {acc_10}')\n",
    "        print(f'Accuracy for y=1, s=1: {acc_11}')       \n",
    "        for i in range(len(sense_gt)):\n",
    "            if sense_gt[i] == 0:\n",
    "                female_predic.append(test_pred[i])\n",
    "                female_gt.append(test_gt[i])\n",
    "            else:\n",
    "                male_predic.append(test_pred[i])\n",
    "                male_gt.append(test_gt[i])\n",
    "        female_CM = confusion_matrix(female_gt, female_predic)    \n",
    "        male_CM = confusion_matrix(male_gt, male_predic) \n",
    "        female_dp = (female_CM[1][1]+female_CM[0][1])/(female_CM[0][0]+female_CM[0][1]+female_CM[1][0]+female_CM[1][1])\n",
    "        male_dp = (male_CM[1][1]+male_CM[0][1])/(male_CM[0][0]+male_CM[0][1]+male_CM[1][0]+male_CM[1][1])\n",
    "        female_TPR = female_CM[1][1]/(female_CM[1][1]+female_CM[1][0])\n",
    "        male_TPR = male_CM[1][1]/(male_CM[1][1]+male_CM[1][0])\n",
    "        female_FPR = female_CM[0][1]/(female_CM[0][1]+female_CM[0][0])\n",
    "        male_FPR = male_CM[0][1]/(male_CM[0][1]+male_CM[0][0])\n",
    "        print('Female TPR', female_TPR)\n",
    "        print('male TPR', male_TPR)\n",
    "        print('DP',abs(female_dp - male_dp))\n",
    "        print('EOP', abs(female_TPR - male_TPR))\n",
    "        print('EoD',0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR)))\n",
    "        print('acc', accuracy_score(test_gt, test_pred))\n",
    "        print('Trade off',accuracy_score(test_gt, test_pred)*(1-0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR))) )\n",
    "\n",
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
