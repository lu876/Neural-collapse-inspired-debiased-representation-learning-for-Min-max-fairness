{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6649b381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from math import log, sqrt, pi\n",
    "import argparse\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable, grad\n",
    "from scipy import linalg as la\n",
    "import math\n",
    "import torchvision.transforms as tvt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import utils\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import trange\n",
    "from transformers import ViTModel\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 224\n",
    "batch_size = 64\n",
    "torch.set_num_threads(1)   # Sets the number of threads used for intra-operations\n",
    "torch.set_num_interop_threads(1)   # Sets the number of threads used for inter-operations\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Changes the seed for reproducibility. \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    \n",
    "class ConfounderDataset(Dataset):\n",
    "    def __init__(self, root_dir,\n",
    "                 target_name, confounder_names,\n",
    "                 model_type=None, augment_data=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == 'Train':\n",
    "            return len(self.training_sample)\n",
    "        if self.split == 'Valid':\n",
    "            return len(self.valid_sample)\n",
    "        if self.split == 'Test':\n",
    "            return len(self.test_sample)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == 'Train': \n",
    "            y = self.training_sample_y_array[idx]\n",
    "            a = self.training_sample_confounder_array[idx]\n",
    "            img_filename = os.path.join(\n",
    "                self.data_dir,\n",
    "                self.training_sample[idx])       \n",
    "            img = Image.open(img_filename).convert('RGB')\n",
    "            img = self.train_transform(img)\n",
    "            x = img\n",
    "            \n",
    "        if self.split == 'Valid': \n",
    "            y = self.valid_sample_y_array[idx]\n",
    "            a = self.valid_sample_confounder_array[idx]\n",
    "            img_filename = os.path.join(\n",
    "                self.data_dir,\n",
    "                self.valid_sample[idx])       \n",
    "            img = Image.open(img_filename).convert('RGB')\n",
    "            img = self.eval_transform(img)\n",
    "            x = img\n",
    "            \n",
    "        if self.split == 'Test': \n",
    "            y = self.test_sample_y_array[idx]\n",
    "            a = self.test_sample_confounder_array[idx]\n",
    "            img_filename = os.path.join(\n",
    "                self.data_dir,\n",
    "                self.test_sample[idx])       \n",
    "            img = Image.open(img_filename).convert('RGB')\n",
    "            img = self.eval_transform(img)\n",
    "            x = img\n",
    "        return x,y,a\n",
    "\n",
    "    \n",
    "    \n",
    "class CUBDataset(ConfounderDataset):\n",
    "    \"\"\"\n",
    "    CUB dataset (already cropped and centered).\n",
    "    Note: metadata_df is one-indexed.\n",
    "    \"\"\"\n",
    "    def __init__(self, fold_dir, split):\n",
    "        self.data_dir = fold_dir\n",
    "        self.split = split\n",
    "\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.data_dir} does not exist yet. Please generate the dataset first.')\n",
    "\n",
    "        # Read in metadata\n",
    "        self.metadata_df = pd.read_csv(\n",
    "            os.path.join(self.data_dir, 'metadata.csv'))\n",
    "\n",
    "        # Get the y values\n",
    "        self.y_array = self.metadata_df['y'].values\n",
    "        self.n_classes = 2\n",
    "\n",
    "        # We only support one confounder for CUB for now\n",
    "        self.confounder_array = self.metadata_df['place'].values\n",
    "        self.n_confounders = 1\n",
    "        \n",
    "        # Extract filenames and splits\n",
    "        self.filename_array = self.metadata_df['img_filename'].values\n",
    "        self.split_array = self.metadata_df['split'].values\n",
    "\n",
    "        self.training_sample = self.filename_array[self.split_array == 0]\n",
    "        self.training_sample_y_array = self.y_array[self.split_array == 0]\n",
    "        self.training_sample_confounder_array =self.confounder_array[self.split_array == 0]\n",
    "        \n",
    "        self.valid_sample = self.filename_array[self.split_array == 1]\n",
    "        self.valid_sample_y_array = self.y_array[self.split_array == 1]\n",
    "        self.valid_sample_confounder_array =self.confounder_array[self.split_array == 1]\n",
    "        \n",
    "        self.test_sample = self.filename_array[self.split_array == 2]\n",
    "        self.test_sample_y_array = self.y_array[self.split_array == 2]\n",
    "        self.test_sample_confounder_array =self.confounder_array[self.split_array == 2]\n",
    "        \n",
    "        # Set transform\n",
    "        self.train_transform = get_transform_cub(aug = True)\n",
    "        self.eval_transform = get_transform_cub(aug = False)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def get_transform_cub(aug):\n",
    "    if aug == False:\n",
    "        transform=tvt.Compose([tvt.Resize((256,256)),\n",
    "                               tvt.CenterCrop((224,224)),\n",
    "                                tvt.ToTensor(),\n",
    "                                tvt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                  \n",
    "                                ])\n",
    "    if aug == True:\n",
    "        transform=tvt.Compose([tvt.Resize((256,256)),\n",
    "                               tvt.RandomResizedCrop(\n",
    "                                    (224,224),\n",
    "                                    scale=(0.7, 1.0),\n",
    "                                    ratio=(0.75, 1.3333333333333333),\n",
    "                                    interpolation=2),\n",
    "                                tvt.RandomHorizontalFlip(),\n",
    "                                tvt.ToTensor(),\n",
    "                                tvt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                  \n",
    "                                ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "seed_everything(2048)\n",
    "\n",
    "fold_dir = r'../../../Dataset/data/waterbirds_v1.0'\n",
    "\n",
    "training_dataset = CUBDataset(fold_dir, 'Train')\n",
    "valid_dataset = CUBDataset(fold_dir, 'Valid')\n",
    "test_dataset = CUBDataset(fold_dir, 'Test')\n",
    "\n",
    "training_data_loader  = torch.utils.data.DataLoader(dataset = training_dataset,\n",
    "                                                batch_size= batch_size,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=0)\n",
    "\n",
    "valid_data_loader  = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
    "                                                batch_size= batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=0)\n",
    "\n",
    "test_data_loader  = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                                batch_size= batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=0)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32987999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0.000000 : 100%|█████████████████████████████████████████████████| 75/75 [00:39<00:00,  1.90batch/s, ut_loss=3.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 14.23939424753189\n",
      "loss g2 (label=0, sensitive=1): 40.728896737098694\n",
      "loss g3 (label=1, sensitive=0): 24.12735202908516\n",
      "loss g4 (label=1, sensitive=1): 7.331673249602318\n",
      "mean loss: 0.28353312611579895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:34<00:00,  2.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9015521064301553 2255.0\n",
      "Accuracy for y=0, s=1: 0.22660753880266074 2255.0\n",
      "Accuracy for y=1, s=0: 0.7336448598130841 642.0\n",
      "Accuracy for y=1, s=1: 0.9766355140186916 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1.000000 : 100%|█████████████████████████████████████████████████| 75/75 [00:39<00:00,  1.90batch/s, ut_loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 16.319543719291687\n",
      "loss g2 (label=0, sensitive=1): 28.956861197948456\n",
      "loss g3 (label=1, sensitive=0): 19.14629152417183\n",
      "loss g4 (label=1, sensitive=1): 10.85228756070137\n",
      "mean loss: 0.12337937206029892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:34<00:00,  2.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9192904656319291 2255.0\n",
      "Accuracy for y=0, s=1: 0.4842572062084257 2255.0\n",
      "Accuracy for y=1, s=0: 0.8348909657320872 642.0\n",
      "Accuracy for y=1, s=1: 0.956386292834891 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2.000000 : 100%|█████████████████████████████████████████████████| 75/75 [00:39<00:00,  1.88batch/s, ut_loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 17.041948348283768\n",
      "loss g2 (label=0, sensitive=1): 24.998200684785843\n",
      "loss g3 (label=1, sensitive=0): 15.76689125597477\n",
      "loss g4 (label=1, sensitive=1): 12.04385620355606\n",
      "mean loss: 0.14413385093212128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:34<00:00,  2.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9135254988913526 2255.0\n",
      "Accuracy for y=0, s=1: 0.7090909090909091 2255.0\n",
      "Accuracy for y=1, s=0: 0.881619937694704 642.0\n",
      "Accuracy for y=1, s=1: 0.9361370716510904 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3.000000 : 100%|█████████████████████████████████████████████████| 75/75 [00:39<00:00,  1.89batch/s, ut_loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 13.569884806871414\n",
      "loss g2 (label=0, sensitive=1): 17.315834760665894\n",
      "loss g3 (label=1, sensitive=0): 11.924803789705038\n",
      "loss g4 (label=1, sensitive=1): 9.886371180415154\n",
      "mean loss: 0.09660772979259491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:34<00:00,  2.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.929490022172949 2255.0\n",
      "Accuracy for y=0, s=1: 0.825720620842572 2255.0\n",
      "Accuracy for y=1, s=0: 0.881619937694704 642.0\n",
      "Accuracy for y=1, s=1: 0.9143302180685359 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4.000000 : 100%|█████████████████████████████████████████████████| 75/75 [00:38<00:00,  1.93batch/s, ut_loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 13.04508301615715\n",
      "loss g2 (label=0, sensitive=1): 13.768602430820465\n",
      "loss g3 (label=1, sensitive=0): 10.40664018318057\n",
      "loss g4 (label=1, sensitive=1): 10.272466398775578\n",
      "mean loss: 0.1688229888677597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:34<00:00,  2.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9312638580931264 2255.0\n",
      "Accuracy for y=0, s=1: 0.8620842572062084 2255.0\n",
      "Accuracy for y=1, s=0: 0.8956386292834891 642.0\n",
      "Accuracy for y=1, s=1: 0.9065420560747663 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5.000000 : 100%|████████████████████████████████████████████████████| 75/75 [00:39<00:00,  1.91batch/s, ut_loss=3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 13.061817929148674\n",
      "loss g2 (label=0, sensitive=1): 14.918389037251472\n",
      "loss g3 (label=1, sensitive=0): 9.344527230598032\n",
      "loss g4 (label=1, sensitive=1): 10.782147891819477\n",
      "mean loss: 0.28314292430877686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:33<00:00,  2.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.926829268292683 2255.0\n",
      "Accuracy for y=0, s=1: 0.8762749445676274 2255.0\n",
      "Accuracy for y=1, s=0: 0.9034267912772586 642.0\n",
      "Accuracy for y=1, s=1: 0.9127725856697819 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6.000000 : 100%|█████████████████████████████████████████████████| 75/75 [00:39<00:00,  1.91batch/s, ut_loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 9.5089121311903\n",
      "loss g2 (label=0, sensitive=1): 9.001399122178555\n",
      "loss g3 (label=1, sensitive=0): 6.103542195633054\n",
      "loss g4 (label=1, sensitive=1): 7.389763401821256\n",
      "mean loss: 0.17090390622615814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:34<00:00,  2.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9392461197339246 2255.0\n",
      "Accuracy for y=0, s=1: 0.8975609756097561 2255.0\n",
      "Accuracy for y=1, s=0: 0.9018691588785047 642.0\n",
      "Accuracy for y=1, s=1: 0.9049844236760125 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7.000000 : 100%|████████████████████████████████████████████████| 75/75 [00:39<00:00,  1.92batch/s, ut_loss=0.751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 9.55667433142662\n",
      "loss g2 (label=0, sensitive=1): 9.333072591573\n",
      "loss g3 (label=1, sensitive=0): 5.987743039615452\n",
      "loss g4 (label=1, sensitive=1): 8.880803797394037\n",
      "mean loss: 0.13809359073638916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:34<00:00,  2.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9419068736141907 2255.0\n",
      "Accuracy for y=0, s=1: 0.9042128603104213 2255.0\n",
      "Accuracy for y=1, s=0: 0.9018691588785047 642.0\n",
      "Accuracy for y=1, s=1: 0.9034267912772586 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8.000000 : 100%|████████████████████████████████████████████████| 75/75 [00:39<00:00,  1.92batch/s, ut_loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 8.591044679284096\n",
      "loss g2 (label=0, sensitive=1): 8.075935654342175\n",
      "loss g3 (label=1, sensitive=0): 4.3606880735605955\n",
      "loss g4 (label=1, sensitive=1): 7.833419468253851\n",
      "mean loss: 0.13173522055149078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:33<00:00,  2.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9427937915742793 2255.0\n",
      "Accuracy for y=0, s=1: 0.9015521064301553 2255.0\n",
      "Accuracy for y=1, s=0: 0.9018691588785047 642.0\n",
      "Accuracy for y=1, s=1: 0.9080996884735203 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9.000000 : 100%|████████████████████████████████████████████████| 75/75 [00:39<00:00,  1.91batch/s, ut_loss=0.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 7.255226723849773\n",
      "loss g2 (label=0, sensitive=1): 6.176181860268116\n",
      "loss g3 (label=1, sensitive=0): 3.186445157160051\n",
      "loss g4 (label=1, sensitive=1): 7.921655707061291\n",
      "mean loss: 0.17130237817764282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:34<00:00,  2.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9490022172949002 2255.0\n",
      "Accuracy for y=0, s=1: 0.9099778270509978 2255.0\n",
      "Accuracy for y=1, s=0: 0.8987538940809969 642.0\n",
      "Accuracy for y=1, s=1: 0.9096573208722741 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10.000000 : 100%|█████████████████████████████████████████████████| 75/75 [00:39<00:00,  1.90batch/s, ut_loss=0.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 7.73557261377573\n",
      "loss g2 (label=0, sensitive=1): 9.176913149654865\n",
      "loss g3 (label=1, sensitive=0): 3.622856927337125\n",
      "loss g4 (label=1, sensitive=1): 8.561302773654461\n",
      "mean loss: 0.17430004477500916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:33<00:00,  2.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9441241685144124 2255.0\n",
      "Accuracy for y=0, s=1: 0.89490022172949 2255.0\n",
      "Accuracy for y=1, s=0: 0.9018691588785047 642.0\n",
      "Accuracy for y=1, s=1: 0.9127725856697819 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11.000000 : 100%|███████████████████████████████████████████████| 75/75 [00:38<00:00,  1.96batch/s, ut_loss=0.798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 6.308450289070606\n",
      "loss g2 (label=0, sensitive=1): 4.691736813634634\n",
      "loss g3 (label=1, sensitive=0): 2.0967680900357664\n",
      "loss g4 (label=1, sensitive=1): 7.503372246399522\n",
      "mean loss: 0.15010923147201538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:33<00:00,  2.72batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9605321507760533 2255.0\n",
      "Accuracy for y=0, s=1: 0.9215077605321508 2255.0\n",
      "Accuracy for y=1, s=0: 0.8769470404984424 642.0\n",
      "Accuracy for y=1, s=1: 0.8987538940809969 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 12.000000 : 100%|████████████████████████████████████████████████| 75/75 [00:39<00:00,  1.91batch/s, ut_loss=0.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 6.04976149648428\n",
      "loss g2 (label=0, sensitive=1): 4.350595861673355\n",
      "loss g3 (label=1, sensitive=0): 2.327973148669116\n",
      "loss g4 (label=1, sensitive=1): 6.46963189356029\n",
      "mean loss: 0.1592898964881897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:34<00:00,  2.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9560975609756097 2255.0\n",
      "Accuracy for y=0, s=1: 0.9095343680709534 2255.0\n",
      "Accuracy for y=1, s=0: 0.8925233644859814 642.0\n",
      "Accuracy for y=1, s=1: 0.9112149532710281 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 13.000000 : 100%|███████████████████████████████████████████████| 75/75 [00:39<00:00,  1.88batch/s, ut_loss=0.631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 6.564943306148052\n",
      "loss g2 (label=0, sensitive=1): 5.250527855008841\n",
      "loss g3 (label=1, sensitive=0): 1.9204386083874851\n",
      "loss g4 (label=1, sensitive=1): 7.341213021427393\n",
      "mean loss: 0.1433398574590683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:34<00:00,  2.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9596452328159645 2255.0\n",
      "Accuracy for y=0, s=1: 0.9108647450110865 2255.0\n",
      "Accuracy for y=1, s=0: 0.8847352024922118 642.0\n",
      "Accuracy for y=1, s=1: 0.9127725856697819 642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 14.000000 : 100%|███████████████████████████████████████████████| 75/75 [00:39<00:00,  1.89batch/s, ut_loss=0.419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss g1 (label=0, sensitive=0): 6.087067548185587\n",
      "loss g2 (label=0, sensitive=1): 5.20187709107995\n",
      "loss g3 (label=1, sensitive=0): 1.096060387964826\n",
      "loss g4 (label=1, sensitive=1): 6.691425814293325\n",
      "mean loss: 0.14121735095977783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:33<00:00,  2.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9662971175166297 2255.0\n",
      "Accuracy for y=0, s=1: 0.9263858093126386 2255.0\n",
      "Accuracy for y=1, s=0: 0.8800623052959502 642.0\n",
      "Accuracy for y=1, s=1: 0.9080996884735203 642.0\n",
      "************Evaluation************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 91/91 [00:34<00:00,  2.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for y=0, s=0: 0.9419068736141907 2255.0\n",
      "Accuracy for y=0, s=1: 0.9042128603104213 2255.0\n",
      "Accuracy for y=1, s=0: 0.9018691588785047 642.0\n",
      "Accuracy for y=1, s=1: 0.9034267912772586 642.0\n",
      "DP 0.029685881946841575\n",
      "EOP 0.0015576323987538387\n",
      "EoD 0.019625822851261625\n",
      "acc 0.918536416983086\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "#import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def compute_variance(g1, g2, g3, g4):\n",
    "    g1_tensor = torch.from_numpy(g1)\n",
    "    g2_tensor = torch.from_numpy(g2)\n",
    "    g3_tensor = torch.from_numpy(g3)\n",
    "    g4_tensor = torch.from_numpy(g4)\n",
    "    sample_feature = np.concatenate((g1_tensor, g2_tensor, g3_tensor, g4_tensor), axis=0)\n",
    "    sample_tensor = torch.from_numpy(sample_feature)\n",
    "    \n",
    "    cov_g1 = torch.cov(g1_tensor.T)\n",
    "    cov_g2 = torch.cov(g2_tensor.T)\n",
    "    cov_g3 = torch.cov(g3_tensor.T)\n",
    "    cov_g4 = torch.cov(g4_tensor.T)\n",
    "    \n",
    "    cov_all = torch.cov(sample_tensor.T)\n",
    "    g1_trace = torch.trace(cov_g1)\n",
    "    g2_trace = torch.trace(cov_g2)\n",
    "    g3_trace = torch.trace(cov_g3)\n",
    "    g4_trace = torch.trace(cov_g4)\n",
    "    \n",
    "    all_trace = torch.trace(cov_all)\n",
    "    \n",
    "    print(cov_g1.shape)\n",
    "    print(cov_all.shape)\n",
    "    print(\"g1 trace\",g1_trace,\"g2 trace\",g2_trace,\"g3 trace\",g3_trace,\"g4 trace\",g4_trace,\"ALL trace\",all_trace)\n",
    "    print(\"g1\",g1_trace/all_trace,\"g2\",g2_trace/all_trace,\"g3\",g3_trace/all_trace,\"g4\",g4_trace/all_trace)\n",
    "\n",
    "    \n",
    "def neural_collapse():\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    model.load_state_dict(torch.load('ICML_weight/WB_ResNet_50_model_epoch_100.pth'))\n",
    "    model.fc = nn.Identity()\n",
    "    model.to(device)   \n",
    "    feature_list = []\n",
    "    feature_y_0_a0 = []\n",
    "    feature_y_0_a1 = []\n",
    "    feature_y_1_a0 = []\n",
    "    feature_y_1_a1 = []\n",
    "    \n",
    "    for step, (test_input, test_target, sensitive) in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n",
    "        with torch.no_grad():\n",
    "            test_input = test_input.to(device)\n",
    "            test_pred_ = model(test_input)\n",
    "            label = test_target.squeeze().detach().cpu()\n",
    "            mask_00 = ((label == 0) & (sensitive == 0))\n",
    "            mask_01 = ((label == 0) & (sensitive == 1))\n",
    "            mask_10 = ((label == 1) & (sensitive == 0))\n",
    "            mask_11 = ((label == 1) & (sensitive == 1))\n",
    "            g1 = test_pred_[mask_00]\n",
    "            g2 = test_pred_[mask_01]\n",
    "            g3 = test_pred_[mask_10]\n",
    "            g4 = test_pred_[mask_11]\n",
    "            feature_y_0_a0.extend(g1.detach().cpu().numpy())\n",
    "            feature_y_0_a1.extend(g2.detach().cpu().numpy())\n",
    "            feature_y_1_a0.extend(g3.detach().cpu().numpy())\n",
    "            feature_y_1_a1.extend(g4.detach().cpu().numpy())\n",
    "\n",
    "            \n",
    "    feature_g1 = np.array(feature_y_0_a0)\n",
    "    feature_g2 = np.array(feature_y_0_a1)\n",
    "    feature_g3 = np.array(feature_y_1_a0)\n",
    "    feature_g4 = np.array(feature_y_1_a1)\n",
    "    print(feature_g1.shape, feature_g2.shape,feature_g3.shape,feature_g4.shape)\n",
    "    \n",
    "    compute_variance(feature_g1,feature_g2,feature_g3,feature_g4)\n",
    "\n",
    "\n",
    "def test(net, classifier, dataloader, print_fairness=True):\n",
    "    net.eval()\n",
    "    test_pred = []\n",
    "    test_gt = []\n",
    "    sense_gt = []\n",
    "    female_predic = []\n",
    "    female_gt = []\n",
    "    male_predic = []\n",
    "    male_gt = []\n",
    "    correct_00, total_00 = 0, 0\n",
    "    correct_01, total_01 = 0, 0\n",
    "    correct_10, total_10 = 0, 0\n",
    "    correct_11, total_11 = 0, 0\n",
    "    classifier = classifier.to(device)\n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "            for content in tepoch:\n",
    "                test, label, sensitive = content\n",
    "                test = test.to(device)\n",
    "                \n",
    "                mask_00 = ((label == 0) & (sensitive == 0))\n",
    "                mask_01 = ((label == 0) & (sensitive == 1))\n",
    "                mask_10 = ((label == 1) & (sensitive == 0))\n",
    "                mask_11 = ((label == 1) & (sensitive == 1))\n",
    "                test_feature = net(test)\n",
    "                prediction = classifier(test_feature)\n",
    "                _, predic = torch.max(prediction.data, 1)\n",
    "                predic = predic.detach().cpu()\n",
    "                test_pred.extend(predic.numpy())\n",
    "                test_gt.extend(label)\n",
    "                sense_gt.extend(sensitive)\n",
    "                correct_00 += (predic[mask_00] == label[mask_00]).float().sum().item()\n",
    "                total_00 += mask_00.float().sum().item()\n",
    "\n",
    "                correct_01 += (predic[mask_01] == label[mask_01]).float().sum().item()\n",
    "                total_01 += mask_01.float().sum().item()\n",
    "\n",
    "                correct_10 += (predic[mask_10] == label[mask_10]).float().sum().item()\n",
    "                total_10 += mask_10.float().sum().item()\n",
    "\n",
    "                correct_11 += (predic[mask_11] == label[mask_11]).float().sum().item()\n",
    "                total_11 += mask_11.float().sum().item() \n",
    "                \n",
    "        acc_00 = correct_00 / total_00\n",
    "        acc_01 = correct_01 / total_01\n",
    "        acc_10 = correct_10 / total_10\n",
    "        acc_11 = correct_11 / total_11\n",
    "\n",
    "        print(f'Accuracy for y=0, s=0: {acc_00}', total_00)\n",
    "        print(f'Accuracy for y=0, s=1: {acc_01}', total_01)\n",
    "        print(f'Accuracy for y=1, s=0: {acc_10}', total_10)\n",
    "        print(f'Accuracy for y=1, s=1: {acc_11}', total_11)  \n",
    "        wga = min(acc_00, acc_01, acc_10, acc_11)\n",
    "\n",
    "\n",
    "        for i in range(len(sense_gt)):\n",
    "            if sense_gt[i] == 0:\n",
    "                female_predic.append(test_pred[i])\n",
    "                female_gt.append(test_gt[i])\n",
    "            else:\n",
    "                male_predic.append(test_pred[i])\n",
    "                male_gt.append(test_gt[i])\n",
    "\n",
    "        female_CM = confusion_matrix(female_gt, female_predic)    \n",
    "        male_CM = confusion_matrix(male_gt, male_predic) \n",
    "        female_dp = (female_CM[1][1]+female_CM[0][1])/(female_CM[0][0]+female_CM[0][1]+female_CM[1][0]+female_CM[1][1])\n",
    "        male_dp = (male_CM[1][1]+male_CM[0][1])/(male_CM[0][0]+male_CM[0][1]+male_CM[1][0]+male_CM[1][1])\n",
    "        female_TPR = female_CM[1][1]/(female_CM[1][1]+female_CM[1][0])\n",
    "        male_TPR = male_CM[1][1]/(male_CM[1][1]+male_CM[1][0])\n",
    "        female_FPR = female_CM[0][1]/(female_CM[0][1]+female_CM[0][0])\n",
    "        male_FPR = male_CM[0][1]/(male_CM[0][1]+male_CM[0][0])\n",
    "        if print_fairness == True:\n",
    "            EOd = 0.5*(abs(female_FPR-male_FPR)+abs(female_TPR-male_TPR))\n",
    "            print('DP',abs(female_dp - male_dp))\n",
    "            print('EOP', abs(female_TPR - male_TPR))\n",
    "            print('EoD', EOd)\n",
    "            print('acc', accuracy_score(test_gt, test_pred))\n",
    "        else:\n",
    "            EOd = 0.5*(abs(female_FPR-male_FPR)+abs(female_TPR-male_TPR))\n",
    "        return accuracy_score(test_gt, test_pred), EOd, wga\n",
    "\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    epoch =15\n",
    "    weight_decay=1e-3\n",
    "    init_lr=1e-4\n",
    "    momentum_decay = 0.9\n",
    "    schedule = True\n",
    "    resnet50 = models.resnet50(pretrained=True)\n",
    "    resnet50.fc = nn.Identity()\n",
    "    num_classes = 2 \n",
    "    classifier = nn.Linear(2048, num_classes)\n",
    "    model = resnet50.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "    resnet50_parameters = model.parameters()\n",
    "    classifier_parameters = classifier.parameters()\n",
    "    combined_parameters = list(resnet50_parameters) + list(classifier_parameters)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    mean_criterion = nn.MSELoss()\n",
    "    acc = 0\n",
    "    wga = 0\n",
    "    optimizer = optim.SGD(combined_parameters, lr=init_lr, momentum=momentum_decay, weight_decay = weight_decay)\n",
    "    optimizer_2 = optim.SGD(model.parameters(), lr=init_lr, momentum=momentum_decay, weight_decay = weight_decay)\n",
    "    \n",
    "    if schedule == True:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max= epoch)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    for epoches in range(epoch):\n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            feature_y_0_a0 = []\n",
    "            feature_y_0_a1 = []\n",
    "            feature_y_1_a0 = []\n",
    "            feature_y_1_a1 = []\n",
    "            loss00 = 0\n",
    "            loss01 = 0\n",
    "            loss10 = 0\n",
    "            loss11 = 0\n",
    "            \n",
    "            with torch.no_grad(): \n",
    "                for step, (valid_input, valid_target, validsensitive) in enumerate(valid_data_loader):\n",
    "                    valid_input = valid_input.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        valid_feature = model(valid_input)\n",
    "                        label = valid_target.squeeze().detach().cpu()\n",
    "                        mask_00 = ((label == 0) & (validsensitive == 0))\n",
    "                        mask_01 = ((label == 0) & (validsensitive == 1))\n",
    "                        mask_10 = ((label == 1) & (validsensitive == 0))\n",
    "                        mask_11 = ((label == 1) & (validsensitive == 1))\n",
    "                        g1 = valid_feature[mask_00]\n",
    "                        g2 = valid_feature[mask_01]\n",
    "                        g3 = valid_feature[mask_10]\n",
    "                        g4 = valid_feature[mask_11]\n",
    "                        feature_y_0_a0.extend(g1.detach().cpu().numpy())\n",
    "                        feature_y_0_a1.extend(g2.detach().cpu().numpy())\n",
    "                        feature_y_1_a0.extend(g3.detach().cpu().numpy())\n",
    "                        feature_y_1_a1.extend(g4.detach().cpu().numpy())\n",
    "\n",
    "                feature_g1 = np.array(feature_y_0_a0)\n",
    "                feature_g4 = np.array(feature_y_1_a1)\n",
    "                feature_g1_tensor = torch.from_numpy(feature_g1)\n",
    "                feature_g4_tensor = torch.from_numpy(feature_g4)\n",
    "\n",
    "                mu_1 = torch.mean(feature_g1_tensor, 0)\n",
    "                mu_1 = mu_1 /torch.norm(mu_1)\n",
    "                mu_2 = torch.mean(feature_g4_tensor, 0)\n",
    "                mu_2 = mu_2 /torch.norm(mu_2)\n",
    "                weight = torch.cat((mu_1.unsqueeze(0), mu_2.unsqueeze(0)), 0)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    classifier.weight = nn.Parameter(weight)\n",
    "\n",
    "            for train_input, train_target, sensitive in tepoch:\n",
    "                train_input = train_input.to(device)\n",
    "                label = train_target.detach().cpu() \n",
    "                one_hot_labels = F.one_hot(train_target, num_classes=2)\n",
    "                train_target = one_hot_labels.float().to(device)\n",
    "                \n",
    "                feature = model(train_input)\n",
    "                classifier = classifier.to(device)\n",
    "                outputs  = classifier(feature)\n",
    "              \n",
    "                mask_00 = ((label== 0) & (sensitive == 0))\n",
    "                mask_01 = ((label == 0) & (sensitive == 1))\n",
    "                mask_10 = ((label == 1) & (sensitive == 0))\n",
    "                mask_11 = ((label == 1) & (sensitive == 1))\n",
    "   \n",
    "                count_00 = mask_00.sum()\n",
    "                count_01 = mask_01.sum()\n",
    "                count_10 = mask_10.sum()\n",
    "                count_11 = mask_11.sum()\n",
    "                    \n",
    "                if count_01==0 or count_10==0 or count_00 == 0 or count_11 == 0:\n",
    "                    continue    \n",
    "                g1_f = feature[mask_00]\n",
    "                g2_f = feature[mask_01]    \n",
    "                mu1 = torch.mean(g1_f, 0)\n",
    "                mu2 = torch.mean(g2_f, 0)\n",
    "                    \n",
    "                g3_f = feature[mask_10]\n",
    "                g4_f = feature[mask_11]\n",
    "                mu3 = torch.mean(g3_f, 0)\n",
    "                mu4 = torch.mean(g4_f, 0)\n",
    "                \n",
    "                loss_mean = mean_criterion(mu1, mu2) + mean_criterion(mu3, mu4)\n",
    "                \n",
    "                \n",
    "                if count_00 > 0:\n",
    "                    loss_00 = criterion(outputs[mask_00], train_target[mask_00])\n",
    "                    loss00 += loss_00.item()\n",
    "                else:\n",
    "                    loss_00 = torch.tensor(0)\n",
    "                    \n",
    "                if count_01 > 0:\n",
    "                    loss_01 = criterion(outputs[mask_01], train_target[mask_01])\n",
    "                    loss01 += loss_01.item()\n",
    "                else:\n",
    "                    loss_01 = torch.tensor(0)\n",
    "                    \n",
    "                if count_10 > 0:\n",
    "                    loss_10 = criterion(outputs[mask_10], train_target[mask_10])\n",
    "                    loss10 += loss_10.item()\n",
    "                else:\n",
    "                    loss_10 = torch.tensor(0)\n",
    "                    \n",
    "                if count_11 > 0:\n",
    "                    loss_11 = criterion(outputs[mask_11], train_target[mask_11])\n",
    "                    loss11 += loss_11.item()\n",
    "                else:\n",
    "                    loss_11 = torch.tensor(0)\n",
    "\n",
    "                loss = loss_00 + loss_01 + loss_10 + loss_11 + loss_mean               \n",
    "                tepoch.set_postfix(ut_loss = loss.item())                   \n",
    "                optimizer_2.zero_grad()    \n",
    "                loss.backward()\n",
    "                optimizer_2.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)\n",
    "            if schedule:\n",
    "                scheduler.step()\n",
    "                \n",
    "        print(\"loss g1 (label=0, sensitive=0):\",loss00 )\n",
    "        print(\"loss g2 (label=0, sensitive=1):\",loss01 )\n",
    "        print(\"loss g3 (label=1, sensitive=0):\",loss10 )\n",
    "        print(\"loss g4 (label=1, sensitive=1):\",loss11 )\n",
    "        print('mean loss:', loss_mean.item())\n",
    "        _,  _ , wg = test(model, classifier, test_data_loader, False)\n",
    "        if wg > wga:\n",
    "            torch.save(model, 'Waterbirds_model.pth')\n",
    "            torch.save(classifier, 'classifier.pth')\n",
    "            wga = wg\n",
    "        \n",
    "\n",
    "train_model()\n",
    "print('************Evaluation************')\n",
    "resnet50 = models.resnet50(pretrained=False)\n",
    "resnet50.fc = nn.Identity()\n",
    "num_classes = 2 \n",
    "classifier = nn.Linear(2048, num_classes)\n",
    "classifier = torch.load('classifier.pth')\n",
    "model = resnet50\n",
    "model = torch.load('Waterbirds_model.pth')\n",
    "model = model.to(device)\n",
    "classifier = classifier.to(device)\n",
    "model.eval()\n",
    "classifier.eval()\n",
    "_,  _ , _ = test(model, classifier, test_data_loader, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DLcourse)",
   "language": "python",
   "name": "dlcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
