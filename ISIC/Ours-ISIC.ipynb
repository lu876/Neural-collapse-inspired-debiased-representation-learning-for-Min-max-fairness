{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from math import log, sqrt, pi\n",
    "import argparse\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable, grad\n",
    "from scipy import linalg as la\n",
    "import math\n",
    "import torchvision.transforms as tvt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import utils\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import trange\n",
    "from transformers import ViTModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_size = 224\n",
    "batch_size = 32\n",
    "torch.set_num_threads(5)   # Sets the number of threads used for intra-operations\n",
    "torch.set_num_interop_threads(5)   # Sets the number of threads used for inter-operations\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Changes the seed for reproducibility. \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "       \n",
    "        \n",
    "def get_transform_ISIC(aug):\n",
    "    if aug == False:\n",
    "        transform=tvt.Compose([tvt.Resize((256,256)),\n",
    "                               tvt.CenterCrop((224,224)),\n",
    "                                tvt.ToTensor(),\n",
    "                                tvt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                  \n",
    "                                ])\n",
    "    if aug == True:\n",
    "        transform=tvt.Compose([tvt.Resize((256,256)),\n",
    "                               tvt.CenterCrop((224,224)),\n",
    "                               tvt.RandomHorizontalFlip(),\n",
    "                               tvt.RandomVerticalFlip(),\n",
    "                               tvt.RandomResizedCrop(224, scale=(0.75, 1.0)),\n",
    "                               tvt.RandomRotation(45),\n",
    "                               tvt.ToTensor(),\n",
    "                               tvt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                  \n",
    "                                ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "class ConfounderDataset(Dataset):\n",
    "    def __init__(self, root_dir,\n",
    "                 target_name, confounder_names,\n",
    "                 model_type=None, augment_data=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == 'train':\n",
    "            return len(self.training_sample)\n",
    "        if self.split == 'val':\n",
    "            return len(self.valid_sample)\n",
    "        if self.split == 'test':\n",
    "            return len(self.test_sample)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == 'train': \n",
    "            y = self.training_sample_y_array[idx]\n",
    "            y = torch.tensor(y)\n",
    "            a = self.training_sample_confounder_array[idx]\n",
    "            a = torch.tensor(a)\n",
    "            img_filename = os.path.join(\n",
    "                self.data_dir,\n",
    "                self.training_sample[idx]) \n",
    "            img = Image.open(img_filename).convert('RGB')\n",
    "            img = self.train_transform(img)\n",
    "            x = img\n",
    "            \n",
    "        if self.split == 'val': \n",
    "            y = self.valid_sample_y_array[idx]\n",
    "            y = torch.tensor(y)\n",
    "            a = self.valid_sample_confounder_array[idx]\n",
    "            a = torch.tensor(a)\n",
    "            img_filename = os.path.join(\n",
    "                self.data_dir,\n",
    "                self.valid_sample[idx])       \n",
    "            img = Image.open(img_filename).convert('RGB')\n",
    "            img = self.eval_transform(img)\n",
    "            x = img\n",
    "            \n",
    "        if self.split == 'test': \n",
    "            y = self.test_sample_y_array[idx]\n",
    "            a = self.test_sample_confounder_array[idx]\n",
    "            y = torch.tensor(y)\n",
    "            a = torch.tensor(a)\n",
    "            img_filename = os.path.join(\n",
    "                self.data_dir,\n",
    "                self.test_sample[idx])       \n",
    "            img = Image.open(img_filename).convert('RGB')\n",
    "            img = self.eval_transform(img)\n",
    "            x = img\n",
    "        return x,y,a\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class ISICDataset(ConfounderDataset):\n",
    "    def __init__(self, \n",
    "                 root_dir,\n",
    "                 seed,\n",
    "                 split,\n",
    "                 target_name = ['label'], \n",
    "                 confounder_names=['patches'],\n",
    "                 model_type=None,\n",
    "                 augment_data=False,\n",
    "                 mix_up=False,\n",
    "                 group_id=None,\n",
    "                 id_val=True):\n",
    "        self.split = split\n",
    "        self.augment_data = augment_data\n",
    "        self.group_id = group_id\n",
    "        self.mix_up = mix_up\n",
    "        self.model_type = model_type\n",
    "        self.target_name = target_name\n",
    "        self.confounder_names = confounder_names\n",
    "        self.split_dir = osp.join(root_dir, 'trap-sets')\n",
    "        self.data_dir = osp.join(root_dir, 'ISIC2018_Task1-2_Training_Input')\n",
    "        \n",
    "        metadata = {}\n",
    "        metadata['train'] = pd.read_csv(osp.join(self.split_dir, f'isic_annotated_train{seed}.csv'))\n",
    "        if id_val:\n",
    "            test_val_data = pd.read_csv(osp.join(self.split_dir, f'isic_annotated_test{seed}.csv'))\n",
    "            idx_val, idx_test = train_test_split(np.arange(len(test_val_data)), \n",
    "                                                test_size=0.8, random_state=0)\n",
    "            metadata['test'] = test_val_data.iloc[idx_test]\n",
    "            metadata['val'] = test_val_data.iloc[idx_val]\n",
    "        else:\n",
    "            metadata['test'] = pd.read_csv(osp.join(self.split_dir, f'isic_annotated_test{seed}.csv'))\n",
    "            metadata['val'] = pd.read_csv(osp.join(self.split_dir, f'isic_annotated_val{seed}.csv'))\n",
    "            # subtracting two dataframes \n",
    "            metadata_new = metadata['train'].merge(metadata['val'], how='left', indicator=True)\n",
    "            metadata_new = metadata_new[metadata_new['_merge'] == 'left_only']\n",
    "            metadata['train'] = metadata_new.drop(columns=['_merge'])\n",
    "        \n",
    "        self.train_transform = get_transform_ISIC(aug = True)\n",
    "        self.eval_transform = get_transform_ISIC(aug = False)\n",
    "        \n",
    "        self.precomputed = False\n",
    "        self.pretransformed = False\n",
    "        self.n_classes = 2\n",
    "        self.n_confounders = 1\n",
    "        confounder = confounder_names[0]\n",
    "        \n",
    "        self.training_sample = metadata['train']['image'].values\n",
    "        self.training_sample_y_array = metadata['train'][target_name].values\n",
    "        self.training_sample_confounder_array = metadata['train'][confounder].values\n",
    "        \n",
    "        self.valid_sample = metadata['val']['image'].values\n",
    "        self.valid_sample_y_array = metadata['val'][target_name].values\n",
    "        self.valid_sample_confounder_array = metadata['val'][confounder].values\n",
    "        \n",
    "        self.test_sample = metadata['test']['image'].values\n",
    "        self.test_sample_y_array = metadata['test'][target_name].values\n",
    "        self.test_sample_confounder_array = metadata['test'][confounder].values\n",
    "        \n",
    "\n",
    "    \n",
    "data_dir = r\"../../isic\"\n",
    "seed = 1\n",
    "\n",
    "training_isic_dataset  = ISICDataset(data_dir, seed, 'train')\n",
    "valid_isic_dataset  = ISICDataset(data_dir, seed, 'val')\n",
    "test_isic_dataset  = ISICDataset(data_dir, seed, 'test')\n",
    "\n",
    "\n",
    "training_data_loader  = torch.utils.data.DataLoader(dataset = training_isic_dataset,\n",
    "                                                batch_size= batch_size,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=0)\n",
    "\n",
    "valid_data_loader  = torch.utils.data.DataLoader(dataset = valid_isic_dataset,\n",
    "                                                batch_size= batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=0)\n",
    "\n",
    "test_data_loader  = torch.utils.data.DataLoader(dataset = test_isic_dataset,\n",
    "                                                batch_size= batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=0)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d95c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def augment_confusion_matrix(CM, gt):\n",
    "    # Augmenting a 1x1 confusion matrix to a 2x2 matrix\n",
    "    if CM.shape == (1, 1):\n",
    "        augmented_CM = np.zeros((2, 2))\n",
    "        if np.unique(gt).item() == 0:\n",
    "            augmented_CM[0, 0] = CM[0, 0]\n",
    "        else:\n",
    "            augmented_CM[1, 1] = CM[0, 0]\n",
    "        return augmented_CM\n",
    "    return CM\n",
    "    \n",
    "def train_model():\n",
    "    epoch =100\n",
    "    weight_decay=1e-3\n",
    "    init_lr=1e-3\n",
    "    momentum_decay = 0.9\n",
    "    schedule = False\n",
    "    resnet50 = models.resnet50(pretrained=True)\n",
    "    \n",
    "    resnet50.fc = nn.Identity()\n",
    "    num_classes = 2 \n",
    "    classifier = nn.Linear(2048, num_classes)\n",
    "    \n",
    "    # Move the model to the appropriate device\n",
    "    model = resnet50.to(device)\n",
    "    #model.layer4[-1].relu = nn.SELU()\n",
    "    classifier = classifier.to(device)\n",
    "    resnet50_parameters = model.parameters()\n",
    "    classifier_parameters = classifier.parameters()\n",
    "    combined_parameters = list(resnet50_parameters) + list(classifier_parameters)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    mean_criterion = nn.MSELoss()\n",
    "    acc = 0\n",
    "    optimizer = optim.SGD(combined_parameters, lr=init_lr, momentum=momentum_decay, weight_decay = weight_decay)\n",
    "    optimizer_2 = optim.SGD(model.parameters(), lr=init_lr, momentum=momentum_decay, weight_decay = weight_decay)\n",
    "    \n",
    "    if schedule == True:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max= epoch)\n",
    "    else:\n",
    "        scheduler = None\n",
    "        \n",
    "    for epoches in range(epoch):   \n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            feature_y_0_a0 = []\n",
    "            feature_y_0_a1 = []\n",
    "            feature_y_1_a0 = []\n",
    "            feature_y_1_a1 = []\n",
    "            loss00 = 0\n",
    "            loss01 = 0\n",
    "            loss10 = 0\n",
    "            loss11 = 0            \n",
    "            with torch.no_grad(): \n",
    "                for step, (valid_input, valid_target, validsensitive) in enumerate(valid_data_loader):\n",
    "                    valid_input = valid_input.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        valid_feature = model(valid_input)\n",
    "                        label = valid_target.squeeze().detach().cpu()\n",
    "                        mask_00 = ((label == 0) & (validsensitive == 0))\n",
    "                        mask_01 = ((label == 0) & (validsensitive == 1))\n",
    "                        mask_10 = ((label == 1) & (validsensitive == 0))\n",
    "                        mask_11 = ((label == 1) & (validsensitive == 1))\n",
    "                        g1 = valid_feature[mask_00]\n",
    "                        g2 = valid_feature[mask_01]\n",
    "                        g3 = valid_feature[mask_10]\n",
    "                        g4 = valid_feature[mask_11]\n",
    "                        feature_y_0_a0.extend(g1.detach().cpu().numpy())\n",
    "                        feature_y_0_a1.extend(g2.detach().cpu().numpy())\n",
    "                        feature_y_1_a0.extend(g3.detach().cpu().numpy())\n",
    "                        feature_y_1_a1.extend(g4.detach().cpu().numpy())\n",
    "                        \n",
    "\n",
    "                feature_g1 = np.array(feature_y_0_a0)\n",
    "                feature_g3 = np.array(feature_y_1_a0)\n",
    "                feature_g1_tensor = torch.from_numpy(feature_g1)\n",
    "                feature_g3_tensor = torch.from_numpy(feature_g3)\n",
    "\n",
    "                mu_1 = torch.mean(feature_g1_tensor, 0)\n",
    "                mu_1 = mu_1 /torch.norm(mu_1)\n",
    "                mu_2 = torch.mean(feature_g3_tensor, 0)\n",
    "                mu_2 = mu_2 /torch.norm(mu_2)\n",
    "                weight = torch.cat((mu_1.unsqueeze(0), mu_2.unsqueeze(0)), 0)\n",
    "                print(weight,\"sim:\",  F.cosine_similarity(mu_1.unsqueeze(0), mu_2.unsqueeze(0)) )\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    classifier.weight = nn.Parameter(weight)           \n",
    "            \n",
    "            for train_input, train_target, sensitive in tepoch:\n",
    "                train_input = train_input.to(device)\n",
    "                label = train_target.squeeze().detach().cpu() \n",
    "                sensitive = sensitive.squeeze().detach().cpu() \n",
    "                one_hot_labels = F.one_hot(train_target.squeeze(), num_classes=2)\n",
    "                train_target = one_hot_labels.float().to(device)\n",
    "                \n",
    "                feature = model(train_input)\n",
    "                classifier = classifier.to(device)\n",
    "                outputs  = classifier(feature)\n",
    "\n",
    "                mask_00 = ((label== 0) & (sensitive == 0))\n",
    "                mask_01 = ((label == 0) & (sensitive == 1))\n",
    "                mask_10 = ((label == 1) & (sensitive == 0))\n",
    "                mask_11 = ((label == 1) & (sensitive == 1))\n",
    "                \n",
    "                count_00 = mask_00.sum()\n",
    "                count_01 = mask_01.sum()\n",
    "                count_10 = mask_10.sum()\n",
    "                count_11 = mask_11.sum()\n",
    "                \n",
    "                g1_f = feature[mask_00]\n",
    "                g2_f = feature[mask_01]               \n",
    "                mu1 = torch.mean(g1_f, 0)\n",
    "                mu2 = torch.mean(g2_f, 0)    \n",
    "                \n",
    "                g3_f = feature[mask_10]\n",
    "                g4_f = feature[mask_11]               \n",
    "                mu3 = torch.mean(g3_f, 0)\n",
    "                mu4 = torch.mean(g4_f, 0)\n",
    "                \n",
    "                if count_00 > 0 and count_01 >0:\n",
    "                    l1 = mean_criterion(mu1, mu2)\n",
    "                else:\n",
    "                    l1 = torch.tensor(0)\n",
    "                    \n",
    "                if count_10 > 0 and count_11 >0:\n",
    "                    l2 = mean_criterion(mu3, mu4)\n",
    "                else:\n",
    "                    l2 = torch.tensor(0)\n",
    "                \n",
    "                loss_mean = l1 + l2\n",
    "                \n",
    "                \n",
    "                if count_00 > 0:\n",
    "                    loss_00 = criterion(outputs[mask_00], train_target[mask_00])\n",
    "                    loss00 += loss_00.item()\n",
    "                else:\n",
    "                    loss_00 = torch.tensor(0)\n",
    "                if count_01 > 0:\n",
    "                    loss_01 = criterion(outputs[mask_01], train_target[mask_01])\n",
    "                    loss01 += loss_01.item()\n",
    "                else:\n",
    "                    loss_01 = torch.tensor(0)\n",
    "                if count_10 > 0:\n",
    "                    loss_10 = criterion(outputs[mask_10], train_target[mask_10])\n",
    "                    loss10 += loss_10.item()\n",
    "                else:\n",
    "                    loss_10 = torch.tensor(0)\n",
    "                if count_11 > 0:\n",
    "                    loss_11 = criterion(outputs[mask_11], train_target[mask_11])\n",
    "                    loss11 += loss_11.item()\n",
    "                else:\n",
    "                    loss_11 = torch.tensor(0)\n",
    "\n",
    "                loss = loss_00 + loss_01 + loss_10 + loss_11 + loss_mean\n",
    "                tepoch.set_postfix(ut_loss = loss.item())\n",
    "                optimizer_2.zero_grad()    \n",
    "                loss.backward()\n",
    "                optimizer_2.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)\n",
    "            if schedule:\n",
    "                scheduler.step()\n",
    "                       \n",
    "        print(\"loss g1 (label=0, sensitive=0):\",loss00 )\n",
    "        print(\"loss g2 (label=0, sensitive=1):\",loss01 )\n",
    "        print(\"loss g3 (label=1, sensitive=0):\",loss10 )\n",
    "        print(\"loss g4 (label=1, sensitive=1):\",loss11 )\n",
    "        print('mean loss:', loss_mean.item())\n",
    "        \n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_prop = []\n",
    "        test_gt = []\n",
    "        sense_gt = []\n",
    "        female_predic = []\n",
    "        female_gt = []\n",
    "        male_predic = []\n",
    "        male_gt = []\n",
    "        correct_00, total_00 = 0, 0\n",
    "        correct_01, total_01 = 0, 0\n",
    "        correct_10, total_10 = 0, 0\n",
    "        correct_11, total_11 = 0, 0\n",
    "        for step, (test_input, test_target, sensitive) in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n",
    "            test_input = test_input.to(device)\n",
    "            test_target = test_target.squeeze()\n",
    "            sensitive = sensitive.squeeze()\n",
    "            gt = test_target.detach().cpu().numpy()\n",
    "            sen = sensitive.detach().cpu().numpy()\n",
    "            test_gt.extend(gt)\n",
    "            sense_gt.extend(sen)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_feature = model(test_input)\n",
    "                test_pred_  = classifier(test_feature)\n",
    "                test_prop_ = F.softmax(test_pred_, 1)[:,1].view(-1)\n",
    "\n",
    "                _, predic = torch.max(test_pred_.data, 1)\n",
    "                predic = predic.detach().cpu()\n",
    "                test_prop_ = test_prop_.detach().cpu()\n",
    "                test_pred.extend(predic.numpy())\n",
    "                test_prop.extend(test_prop_.numpy())\n",
    "                label = test_target.squeeze().detach().cpu()\n",
    "                mask_00 = ((label == 0) & (sensitive == 0))\n",
    "                mask_01 = ((label == 0) & (sensitive == 1))\n",
    "                mask_10 = ((label == 1) & (sensitive == 0))\n",
    "                mask_11 = ((label == 1) & (sensitive == 1))\n",
    "\n",
    "                correct_00 += (predic[mask_00] == label[mask_00]).float().sum().item()\n",
    "                total_00 += mask_00.float().sum().item()\n",
    "\n",
    "                correct_01 += (predic[mask_01] == label[mask_01]).float().sum().item()\n",
    "                total_01 += mask_01.float().sum().item()\n",
    "\n",
    "                correct_10 += (predic[mask_10] == label[mask_10]).float().sum().item()\n",
    "                total_10 += mask_10.float().sum().item()\n",
    "\n",
    "                correct_11 += (predic[mask_11] == label[mask_11]).float().sum().item()\n",
    "                total_11 += mask_11.float().sum().item() \n",
    "        if total_11 == 0:\n",
    "            acc_11 = 0\n",
    "        else:\n",
    "            acc_11 = correct_11 / total_11\n",
    "            \n",
    "        acc_00 = correct_00 / total_00\n",
    "        acc_01 = correct_01 / total_01\n",
    "        acc_10 = correct_10 / total_10\n",
    "\n",
    "        print(f'Accuracy for y=0, s=0: {acc_00}', total_00)\n",
    "        print(f'Accuracy for y=0, s=1: {acc_01}', total_01)\n",
    "        print(f'Accuracy for y=1, s=0: {acc_10}', total_10)\n",
    "        print(f'Accuracy for y=1, s=1: {acc_11}', total_11)  \n",
    "        roc = roc_auc_score(test_gt, test_prop)\n",
    "        print('**************')\n",
    "        print('ROC:', roc)\n",
    "        for i in range(len(sense_gt)):\n",
    "            if sense_gt[i] == 0:\n",
    "                female_predic.append(test_pred[i])\n",
    "                female_gt.append(test_gt[i])\n",
    "            else:\n",
    "                male_predic.append(test_pred[i])\n",
    "                male_gt.append(test_gt[i])\n",
    "        female_CM = confusion_matrix(female_gt, female_predic)    \n",
    "        male_CM = confusion_matrix(male_gt, male_predic) \n",
    "        male_CM = augment_confusion_matrix(male_CM, male_gt)\n",
    "        print('cm for male', male_CM)\n",
    "        female_dp = (female_CM[1][1]+female_CM[0][1])/(female_CM[0][0]+female_CM[0][1]+female_CM[1][0]+female_CM[1][1])\n",
    "        male_dp = (male_CM[1][1]+male_CM[0][1])/(male_CM[0][0]+male_CM[0][1]+male_CM[1][0]+male_CM[1][1])\n",
    "        female_TPR = female_CM[1][1]/(female_CM[1][1]+female_CM[1][0])\n",
    "        male_TPR = male_CM[1][1]/(male_CM[1][1]+male_CM[1][0]+1e-10)\n",
    "        female_FPR = female_CM[0][1]/(female_CM[0][1]+female_CM[0][0])\n",
    "        male_FPR = male_CM[0][1]/(male_CM[0][1]+male_CM[0][0])\n",
    "        if accuracy_score(test_gt, test_pred)> acc:\n",
    "            acc = accuracy_score(test_gt, test_pred)\n",
    "        if acc_00 > 0.7 and acc_10> 0.7:\n",
    "            torch.save(model.state_dict(), f'ISIC_best_ours.pth')\n",
    "            \n",
    "        print('Female TPR', female_TPR)\n",
    "        print('male TPR', male_TPR)\n",
    "        print('DP',abs(female_dp - male_dp))\n",
    "        print('EOP', abs(female_TPR - male_TPR))\n",
    "        print('EoD',0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR)))\n",
    "        print('acc', accuracy_score(test_gt, test_pred))\n",
    "        print('Trade off',accuracy_score(test_gt, test_pred)*(1-0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR))) )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "seed_everything(2048)    \n",
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
