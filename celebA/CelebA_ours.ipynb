{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6649b381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenyu/miniconda3/envs/DLcourse/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/shenyu/miniconda3/envs/DLcourse/lib/python3.7/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 138355\n",
      "Validation dataset size: 24415\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#step 1 import image\n",
    "%matplotlib inline\n",
    "import torchvision.datasets\n",
    "import math\n",
    "import torchvision.transforms as tvt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from transformers import ViTModel\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_id = 1\n",
    "image_size = 224\n",
    "batch_size = 10\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Changes the seed for reproducibility. \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    \n",
    "def train_val_split(dataset, val_percent=0.15):\n",
    "    \"\"\"\n",
    "    Splits a dataset into training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "    - dataset (Dataset): The dataset to split.\n",
    "    - val_percent (float): The percentage of the dataset to use for the validation set.\n",
    "    \n",
    "    Returns:\n",
    "    - train_dataset (Subset): The training dataset.\n",
    "    - val_dataset (Subset): The validation dataset.\n",
    "    \"\"\"\n",
    "    val_size = int(len(dataset) * val_percent)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_transform_celebA(aug):\n",
    "    if aug == False:\n",
    "        transform=tvt.Compose([tvt.Resize((256,256)),\n",
    "                               tvt.CenterCrop((224,224)),\n",
    "                                tvt.ToTensor(),\n",
    "                                tvt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                  \n",
    "                                ])\n",
    "    if aug == True:\n",
    "        transform=tvt.Compose([tvt.Resize((256,256)),\n",
    "                               tvt.RandomResizedCrop(\n",
    "                                    (224,224),\n",
    "                                    scale=(0.7, 1.0),\n",
    "                                    ratio=(0.75, 1.3333333333333333),\n",
    "                                    interpolation=2),\n",
    "                                tvt.RandomHorizontalFlip(),\n",
    "                                tvt.ToTensor(),\n",
    "                                tvt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                  \n",
    "                                ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "seed_everything(1024)\n",
    "\n",
    "dataset = torchvision.datasets.CelebA(\"../../../celeba/datasets/\",split='train', transform=get_transform_celebA(True))\n",
    "test_dataset = torchvision.datasets.CelebA(\"../../../celeba/datasets/\",split='test', transform=get_transform_celebA(False))\n",
    "\n",
    "# Splitting the dataset\n",
    "train_dataset, val_dataset = train_val_split(dataset)\n",
    "\n",
    "# Creating data loaders for training and validation\n",
    "training_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32987999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 13835/13835 [10:45<00:00, 21.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g1: tensor(25303) g2: tensor(41892) g3: tensor(55019) g4: tensor(16136)\n",
      "loss g1 (label=0, sensitive=0): 0\n",
      "loss g2 (label=0, sensitive=1): 0\n",
      "loss g3 (label=1, sensitive=0): 0\n",
      "loss g4 (label=1, sensitive=1): 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2339424/185707529.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2339424/185707529.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss g3 (label=1, sensitive=0):\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss10\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss g4 (label=1, sensitive=1):\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss11\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_mean' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def train_model():\n",
    "    epoch =300\n",
    "    weight_decay=1e-3\n",
    "    init_lr=1e-3\n",
    "    momentum_decay = 0.9\n",
    "    schedule = False\n",
    "    resnet50 = models.resnet50(pretrained=True)\n",
    "    \n",
    "    resnet50.fc = nn.Identity()\n",
    "    num_classes = 2 \n",
    "    classifier = nn.Linear(2048, num_classes)\n",
    "    \n",
    "    # Move the model to the appropriate device\n",
    "    model = resnet50.to(device)\n",
    "    #model.layer4[-1].relu = nn.SELU()\n",
    "    classifier = classifier.to(device)\n",
    "    resnet50_parameters = model.parameters()\n",
    "    classifier_parameters = classifier.parameters()\n",
    "    combined_parameters = list(resnet50_parameters) + list(classifier_parameters)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    mean_criterion = nn.MSELoss()\n",
    "    acc = 0\n",
    "    optimizer = optim.SGD(combined_parameters, lr=init_lr, momentum=momentum_decay, weight_decay = weight_decay)\n",
    "    optimizer_2 = optim.SGD(model.parameters(), lr=init_lr, momentum=momentum_decay, weight_decay = weight_decay)\n",
    "    \n",
    "    if schedule == True:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max= epoch)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \"\"\"\n",
    "    for epoches in range(100):\n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            for train_input, attribute in tepoch:\n",
    "                train_target, sensitive = attribute[:,2], attribute[:,20]\n",
    "                train_input = train_input.to(device)\n",
    "                \n",
    "                one_hot_labels = F.one_hot(train_target, num_classes=2)\n",
    "                train_target = one_hot_labels.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                feature = model(train_input)\n",
    "                outputs  = classifier(feature)\n",
    "                loss = criterion(outputs, train_target)\n",
    "                tepoch.set_postfix(ut_loss = loss.item())\n",
    "                    \n",
    "                    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)\n",
    "            if schedule:\n",
    "                scheduler.step()\n",
    "                \"\"\"\n",
    "                \n",
    "                \n",
    "                \n",
    "    #for name, param in classifier.named_parameters():\n",
    "    #    param.requires_grad = False            \n",
    "    for epoches in range(epoch):\n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            feature_y_0_a0 = []\n",
    "            feature_y_0_a1 = []\n",
    "            feature_y_1_a0 = []\n",
    "            feature_y_1_a1 = []\n",
    "            loss00 = 0\n",
    "            loss01 = 0\n",
    "            loss10 = 0\n",
    "            loss11 = 0\n",
    "            \n",
    "            \n",
    "            with torch.no_grad(): \n",
    "                for step, (valid_input, valid_attributes) in enumerate(valid_data_loader):\n",
    "                    valid_target, validsensitive = valid_attributes[:,2], valid_attributes[:,20]\n",
    "                    valid_input = valid_input.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        valid_feature = model(valid_input)\n",
    "                        label = valid_target.squeeze().detach().cpu()\n",
    "                        mask_00 = ((label == 0) & (validsensitive == 0))\n",
    "                        mask_01 = ((label == 0) & (validsensitive == 1))\n",
    "                        mask_10 = ((label == 1) & (validsensitive == 0))\n",
    "                        mask_11 = ((label == 1) & (validsensitive == 1))\n",
    "                        g1 = valid_feature[mask_00]\n",
    "                        g2 = valid_feature[mask_01]\n",
    "                        g3 = valid_feature[mask_10]\n",
    "                        g4 = valid_feature[mask_11]\n",
    "                        feature_y_0_a0.extend(g1.detach().cpu().numpy())\n",
    "                        feature_y_0_a1.extend(g2.detach().cpu().numpy())\n",
    "                        feature_y_1_a0.extend(g3.detach().cpu().numpy())\n",
    "                        feature_y_1_a1.extend(g4.detach().cpu().numpy())\n",
    "\n",
    "                feature_g2 = np.array(feature_y_0_a1)\n",
    "                feature_g3 = np.array(feature_y_1_a0)\n",
    "                feature_g2_tensor = torch.from_numpy(feature_g2)\n",
    "                feature_g3_tensor = torch.from_numpy(feature_g3)\n",
    "\n",
    "                mu_1 = torch.mean(feature_g2_tensor, 0)\n",
    "                mu_1 = mu_1 /torch.norm(mu_1)\n",
    "                mu_2 = torch.mean(feature_g3_tensor, 0)\n",
    "                mu_2 = mu_2 /torch.norm(mu_2)\n",
    "                weight = torch.cat((mu_1.unsqueeze(0), mu_2.unsqueeze(0)), 0)\n",
    "                print(weight,\"sim:\",  F.cosine_similarity(mu_1.unsqueeze(0), mu_2.unsqueeze(0)) )\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    classifier.weight = nn.Parameter(weight)\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "            for train_input, attribute in tepoch:\n",
    "                train_target, sensitive = attribute[:, 2], attribute[:, 20]\n",
    "                train_input = train_input.to(device)\n",
    "                label = train_target.detach().cpu() \n",
    "                one_hot_labels = F.one_hot(train_target, num_classes=2)\n",
    "                train_target = one_hot_labels.float().to(device)\n",
    "                \n",
    "                feature = model(train_input)\n",
    "                classifier = classifier.to(device)\n",
    "                outputs  = classifier(feature)\n",
    "                \n",
    "                \n",
    "                \n",
    "                mask_00 = ((label== 0) & (sensitive == 0))\n",
    "                mask_01 = ((label == 0) & (sensitive == 1))\n",
    "                mask_10 = ((label == 1) & (sensitive == 0))\n",
    "                mask_11 = ((label == 1) & (sensitive == 1))\n",
    "                \n",
    "                \n",
    "                \n",
    "                count_00 = mask_00.sum()\n",
    "                count_01 = mask_01.sum()\n",
    "                count_10 = mask_10.sum()\n",
    "                count_11 = mask_11.sum()\n",
    "                \n",
    "                if count_01==0 or count_10==0 or count_00 == 0 or count_11 == 0:\n",
    "                    continue\n",
    "                g1_f = feature[mask_00]\n",
    "                g2_f = feature[mask_01]\n",
    "                \n",
    "                mu1 = torch.mean(g1_f, 0)\n",
    "                mu2 = torch.mean(g2_f, 0)\n",
    "                    \n",
    "                g3_f = feature[mask_10]\n",
    "                g4_f = feature[mask_11]\n",
    "                \n",
    "                mu3 = torch.mean(g3_f, 0)\n",
    "                mu4 = torch.mean(g4_f, 0)\n",
    "                \n",
    "                loss_mean = mean_criterion(mu3, mu4) + mean_criterion(mu1, mu2)\n",
    "                \n",
    "                \n",
    "                if count_00 > 0:\n",
    "                    loss_00 = criterion(outputs[mask_00], train_target[mask_00])\n",
    "                    loss00 += loss_00.item()\n",
    "                else:\n",
    "                    loss_00 = torch.tensor(0)\n",
    "                if count_01 > 0:\n",
    "                    loss_01 = criterion(outputs[mask_01], train_target[mask_01])\n",
    "                    loss01 += loss_01.item()\n",
    "                else:\n",
    "                    loss_01 = torch.tensor(0)\n",
    "                if count_10 > 0:\n",
    "                    loss_10 = criterion(outputs[mask_10], train_target[mask_10])\n",
    "                    loss10 += loss_10.item()\n",
    "                else:\n",
    "                    loss_10 = torch.tensor(0)\n",
    "                if count_11 > 0:\n",
    "                    loss_11 = criterion(outputs[mask_11], train_target[mask_11])\n",
    "                    loss11 += loss_11.item()\n",
    "                else:\n",
    "                    loss_11 = torch.tensor(0)\n",
    "\n",
    "                \n",
    "                #loss = criterion(outputs, train_target)\n",
    "                loss = loss_00 + loss_01 + loss_10 + loss_11 + loss_mean\n",
    "                \n",
    "                \n",
    "                tepoch.set_postfix(ut_loss = loss.item())\n",
    "                    \n",
    "                optimizer_2.zero_grad()    \n",
    "                loss.backward()\n",
    "                optimizer_2.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)\n",
    "            if schedule:\n",
    "                scheduler.step()\n",
    "                \n",
    "        print('g1:', total_00, 'g2:', total_01, 'g3:', total_10, 'g4:', total_11)        \n",
    "        print(\"loss g1 (label=0, sensitive=0):\",loss00 )\n",
    "        print(\"loss g2 (label=0, sensitive=1):\",loss01 )\n",
    "        print(\"loss g3 (label=1, sensitive=0):\",loss10 )\n",
    "        print(\"loss g4 (label=1, sensitive=1):\",loss11 )\n",
    "        print('mean loss:', loss_mean.item())\n",
    "        \n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_gt = []\n",
    "        sense_gt = []\n",
    "        female_predic = []\n",
    "        female_gt = []\n",
    "        male_predic = []\n",
    "        male_gt = []\n",
    "        correct_00, total_00 = 0, 0\n",
    "        correct_01, total_01 = 0, 0\n",
    "        correct_10, total_10 = 0, 0\n",
    "        correct_11, total_11 = 0, 0\n",
    "\n",
    "    # Evaluate on test set.\n",
    "        for step, (test_input, attributes) in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n",
    "            sensitive, test_target = attributes[:,20], attributes[:,2]\n",
    "            test_input = test_input.to(device)\n",
    "            test_target = test_target.to(device)\n",
    "\n",
    "            gt = test_target.detach().cpu().numpy()\n",
    "            sen = sensitive.detach().cpu().numpy()\n",
    "            test_gt.extend(gt)\n",
    "            sense_gt.extend(sen)\n",
    "\n",
    "            # Todo: split according to sensitive attribute\n",
    "            # Todo: combine all batch togather\n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_pred_ = model(test_input)\n",
    "                _, predic = torch.max(test_pred_.data, 1)\n",
    "                predic = predic.detach().cpu()\n",
    "                test_pred.extend(predic.numpy())\n",
    "                label = test_target.squeeze().detach().cpu()\n",
    "                mask_00 = ((label == 0) & (sensitive == 0))\n",
    "                mask_01 = ((label == 1) & (sensitive == 0))\n",
    "                mask_10 = ((label == 0) & (sensitive == 1))\n",
    "                mask_11 = ((label == 1) & (sensitive == 1))\n",
    "                \n",
    "                correct_00 += (predic[mask_00] == label[mask_00]).float().sum().item()\n",
    "                total_00 += mask_00.float().sum().item()\n",
    "\n",
    "                correct_01 += (predic[mask_01] == label[mask_01]).float().sum().item()\n",
    "                total_01 += mask_01.float().sum().item()\n",
    "\n",
    "                correct_10 += (predic[mask_10] == label[mask_10]).float().sum().item()\n",
    "                total_10 += mask_10.float().sum().item()\n",
    "\n",
    "                correct_11 += (predic[mask_11] == label[mask_11]).float().sum().item()\n",
    "                total_11 += mask_11.float().sum().item() \n",
    "        acc_00 = correct_00 / total_00\n",
    "        acc_01 = correct_01 / total_01\n",
    "        acc_10 = correct_10 / total_10\n",
    "        acc_11 = correct_11 / total_11\n",
    "\n",
    "        print(f'Accuracy for y=0, s=0: {acc_00}')\n",
    "        print(f'Accuracy for y=0, s=1: {acc_01}')\n",
    "        print(f'Accuracy for y=1, s=0: {acc_10}')\n",
    "        print(f'Accuracy for y=1, s=1: {acc_11}')       \n",
    "        for i in range(len(sense_gt)):\n",
    "            if sense_gt[i] == 0:\n",
    "                female_predic.append(test_pred[i])\n",
    "                female_gt.append(test_gt[i])\n",
    "            else:\n",
    "                male_predic.append(test_pred[i])\n",
    "                male_gt.append(test_gt[i])\n",
    "        female_CM = confusion_matrix(female_gt, female_predic)    \n",
    "        male_CM = confusion_matrix(male_gt, male_predic) \n",
    "        female_dp = (female_CM[1][1]+female_CM[0][1])/(female_CM[0][0]+female_CM[0][1]+female_CM[1][0]+female_CM[1][1])\n",
    "        male_dp = (male_CM[1][1]+male_CM[0][1])/(male_CM[0][0]+male_CM[0][1]+male_CM[1][0]+male_CM[1][1])\n",
    "        female_TPR = female_CM[1][1]/(female_CM[1][1]+female_CM[1][0])\n",
    "        male_TPR = male_CM[1][1]/(male_CM[1][1]+male_CM[1][0])\n",
    "        female_FPR = female_CM[0][1]/(female_CM[0][1]+female_CM[0][0])\n",
    "        male_FPR = male_CM[0][1]/(male_CM[0][1]+male_CM[0][0])\n",
    "        if accuracy_score(test_gt, test_pred)> acc:\n",
    "            acc = accuracy_score(test_gt, test_pred)\n",
    "        print('Female TPR', female_TPR)\n",
    "        print('male TPR', male_TPR)\n",
    "        print('DP',abs(female_dp - male_dp))\n",
    "        print('EOP', abs(female_TPR - male_TPR))\n",
    "        print('EoD',0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR)))\n",
    "        print('acc', accuracy_score(test_gt, test_pred))\n",
    "        print('Trade off',accuracy_score(test_gt, test_pred)*(1-0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR))) )\n",
    "\n",
    "\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1024  \n",
    "Accuracy for y=0, s=0: 0.8015482054890922\n",
    "Accuracy for y=0, s=1: 0.8068637274549099\n",
    "Accuracy for y=1, s=0: 0.7993449405274953\n",
    "Accuracy for y=1, s=1: 0.8375130616509927\n",
    "Female TPR 0.8068637274549099\n",
    "male TPR 0.8375130616509927\n",
    "DP 0.23643253382353846\n",
    "EOP 0.030649334196082845\n",
    "EoD 0.016426299578839898\n",
    "acc 0.8064823164011622\n",
    "Trade off 0.79323479626692\n",
    "0\n",
    "\n",
    "Accuracy for y=0, s=0: 0.8078817733990148\n",
    "Accuracy for y=0, s=1: 0.8167557317703844\n",
    "Accuracy for y=1, s=0: 0.783066132264529\n",
    "Accuracy for y=1, s=1: 0.7789968652037618\n",
    "Female TPR 0.783066132264529\n",
    "male TPR 0.7789968652037618\n",
    "DP 0.24632246361760607\n",
    "EOP 0.004069267060767268\n",
    "EoD 0.006471612716068442\n",
    "acc 0.7977657549343753\n",
    "\n",
    "2048\n",
    "\n",
    "Accuracy for y=0, s=0: 0.7694112127609665\n",
    "Accuracy for y=0, s=1: 0.7764178589898293\n",
    "Accuracy for y=1, s=0: 0.8270290581162325\n",
    "Accuracy for y=1, s=1: 0.8427377220480669\n",
    "Female TPR 0.8270290581162325\n",
    "male TPR 0.8427377220480669\n",
    "DP 0.2422297026091964\n",
    "EOP 0.015708663931834344\n",
    "EoD 0.011357655080348611\n",
    "acc 0.8015228934976455\n",
    "Trade off 0.7924194729342963"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
